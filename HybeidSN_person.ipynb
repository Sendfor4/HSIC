{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchinfo import summary\n",
    "from scipy.io import loadmat\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, cohen_kappa_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from operator import truediv\n",
    "import spectral\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设定变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'IP'  # 指定数集名称\n",
    "test_ratio = 0.7  # 用于测试样本的比例\n",
    "patch_size = 25  # 每个像素周围提取 patch 的尺寸\n",
    "pca_components = 30  # 使用 PCA 降维，得到主成分的数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 载入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(name):\n",
    "    data_path = os.path.join(os.getcwd(),'dataset')\n",
    "    if name == 'IP':\n",
    "        data = loadmat(os.path.join(data_path, 'IndianPines\\\\Indian_pines_corrected.mat'))['indian_pines_corrected']\n",
    "        labels = loadmat(os.path.join(data_path, 'IndianPines\\\\Indian_pines_gt.mat'))['indian_pines_gt']\n",
    "    elif name == 'SA':\n",
    "        data = loadmat(os.path.join(data_path, 'Salinas\\\\Salinas_corrected.mat'))['salinas_corrected']\n",
    "        labels = loadmat(os.path.join(data_path, 'Salinas\\\\Salinas_gt.mat'))['salinas_gt']\n",
    "    elif name == 'PU':\n",
    "        data = loadmat(os.path.join(data_path, 'PaviaU\\\\PaviaU.mat'))['paviaU']\n",
    "        labels = loadmat(os.path.join(data_path, 'PaviaU\\\\PaviaU_gt.mat'))['paviaU_gt']\n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyPCA(X, numComponents=15):\n",
    "    newX = np.reshape(X, (-1, X.shape[2]))\n",
    "    pca = PCA(n_components=numComponents, whiten=True)  # PCA 以及 归一\n",
    "    newX = pca.fit_transform(newX)  # 拟合数据并转换 (145*145, 30)\n",
    "    newX = np.reshape(newX, (X.shape[0],X.shape[1], numComponents))\n",
    "    return newX, pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 填充边缘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padWithZeros(X, margin=2):\n",
    "    newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n",
    "    x_offset = margin\n",
    "    y_offset = margin\n",
    "    newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n",
    "    return newX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建立方块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createImageCubes(X, y, patch_size=5, removeZeroLabels = True):\n",
    "    margin = int((patch_size - 1) / 2)\n",
    "    zeroPaddedX = padWithZeros(X, margin=margin)\n",
    "    # split patches\n",
    "    patchesData = np.zeros((X.shape[0] * X.shape[1], patch_size, patch_size, X.shape[2]))\n",
    "    # (21025, 25, 25, 200)\n",
    "    patchesLabels = np.zeros((X.shape[0] * X.shape[1]))\n",
    "    # (21025, )\n",
    "    patchIndex = 0\n",
    "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
    "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
    "            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]   \n",
    "            # [r - 12 : r + 12 + 1, c -12 : c + 12 + 1]\n",
    "            patchesData[patchIndex, :, :, :] = patch\n",
    "            patchesLabels[patchIndex] = y[r-margin, c-margin]\n",
    "            patchIndex = patchIndex + 1\n",
    "    \n",
    "    # 去掉 gt 标签集 groundtruth 中为 0 的数据\n",
    "    if removeZeroLabels:\n",
    "        patchesData = patchesData[patchesLabels>0,:,:,:]\n",
    "        patchesLabels = patchesLabels[patchesLabels>0]\n",
    "        patchesLabels -= 1\n",
    "        \n",
    "    return patchesData, patchesLabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 载入并处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((145, 145, 200), (145, 145), 21025)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, label = loadData(dataset)\n",
    "data.shape, label.shape, data.shape[0]*data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145, 145, 30)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_components = pca_components if dataset == 'IP' else 15\n",
    "X, pca = applyPCA(data, numComponents=pca_components)\n",
    "y = label\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10249, 25, 25, 30), (10249,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,y = createImageCubes(X, y, patch_size=patch_size)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, \n",
    "                                                y, \n",
    "                                                test_size=test_ratio,\n",
    "                                                random_state=42,\n",
    "                                                stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 展示数据集采样信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  class   train_num  test_num  all_num  \n",
      "    1        428       1000      1428   \n",
      "    2        249       581       830    \n",
      "    3         71       166       237    \n",
      "    4        145       338       483    \n",
      "    5        219       511       730    \n",
      "    6         8         20        28    \n",
      "    7        143       335       478    \n",
      "    8         6         14        20    \n",
      "    9        292       680       972    \n",
      "    10       736       1719      2455   \n",
      "    11       178       415       593    \n",
      "    12        62       143       205    \n",
      "    13       379       886       1265   \n",
      "    14       116       270       386    \n",
      "    15        28        65        93    \n",
      "    16        0         0         0     \n",
      "  total      3060      7143     10203   \n"
     ]
    }
   ],
   "source": [
    "sample_report = f\"{'class': ^10}{'train_num':^10}{'test_num': ^10}{'all_num':^10}\\n\"\n",
    "for i in np.unique(label):\n",
    "    if i == 0: continue\n",
    "    sample_report += f\"{i: ^10}{(ytrain==i).sum(): ^10}{(ytest==i).sum(): ^10}{(y==i).sum(): ^10}\\n\"\n",
    "sample_report += f\"{'total': ^10}{np.count_nonzero(ytrain): ^10}{np.count_nonzero(ytest): ^10}{np.count_nonzero(y): ^10}\"\n",
    "print(sample_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridSN(nn.Module):\n",
    "    def __init__(self, num_classes=16):  # 设置默认分类种类为16类，与Indian Pines数据集的类别数量相同\n",
    "          super(HybridSN, self).__init__()\n",
    "          self.Conv1 = nn.Sequential(\n",
    "                         nn.Conv3d(in_channels=1,out_channels=8,kernel_size=(7, 3, 3)),\n",
    "                         nn.ReLU(inplace=True))\n",
    "          self.Conv2 = nn.Sequential(\n",
    "                         nn.Conv3d(in_channels=8,out_channels=16,kernel_size=(5, 3, 3)),\n",
    "                         nn.ReLU(inplace=True))\n",
    "          self.Conv3 = nn.Sequential(\n",
    "                         nn.Conv3d(in_channels=16,out_channels=32,kernel_size=(3, 3, 3)),\n",
    "                         nn.ReLU(inplace=True))\n",
    "          \n",
    "          self.Conv4 = nn.Sequential(\n",
    "               nn.Conv2d(576, 64, (3, 3)),\n",
    "               nn.BatchNorm2d(64),\n",
    "               nn.ReLU(inplace=True)\n",
    "          )\n",
    "          # 对数据进行展平\n",
    "          self.flatten = nn.Flatten()\n",
    "          # 从 18496 个节点映射到 256 个节点\n",
    "          self.dense1 = nn.Linear(18496, 256)\n",
    "          # 从 256 个节点映射到 128 个节点\n",
    "          self.dense2 = nn.Linear(256, 128)\n",
    "          # 从 256 个节点映射到 分类 的类别\n",
    "          self.dense3 = nn.Linear(128, num_classes)\n",
    "          # 丢失神经元概率为0.5\n",
    "          self.drop = nn.Dropout(p=0.5)\n",
    "\n",
    "    # 前向传播\n",
    "    def forward(self, x):\n",
    "          x = self.Conv1(x)  # 第一层三维卷积\n",
    "          x = self.Conv2(x)  # 第二层三维卷积\n",
    "          x = self.Conv3(x)  # 第三层三维卷积\n",
    "          x = x.reshape(-1, x.shape[1] * x.shape[2], x.shape[3], x.shape[4])\n",
    "          x = self.Conv4(x)  # 二维卷积\n",
    "          x = self.flatten(x)  # 展平\n",
    "          x = F.relu(self.drop(self.dense1(x)))  # 映射\n",
    "          x = F.relu(self.drop(self.dense2(x)))  # 映射\n",
    "          x = self.dense3(x)  # 映射\n",
    "          \n",
    "          return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试网络是否跑通"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16])\n"
     ]
    }
   ],
   "source": [
    "module = HybridSN()\n",
    "input = torch.ones(((1, 1, 30, 25, 25)))\n",
    "output = module(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(module, input_size=(1,30,25,25),col_names=['num_params','kernel_size','mult_adds','input_size','output_size'],col_width=10,row_settings=['var_names'],depth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 迭代数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3074, 1, 30, 25, 25), (7175, 1, 30, 25, 25))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转换数据以适应 pytorch 结构\n",
    "Xtrain = Xtrain.reshape(-1, patch_size, patch_size, pca_components, 1)\n",
    "Xtest  = Xtest.reshape(-1, patch_size, patch_size, pca_components, 1)\n",
    "Xtrain = Xtrain.transpose(0, 4, 3, 1, 2)\n",
    "Xtest  = Xtest.transpose(0, 4, 3, 1, 2)\n",
    "Xtrain.shape,Xtest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建可以迭代的训练集和测试集类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.len = Xtrain.shape[0]\n",
    "        self.X_data = torch.FloatTensor(Xtrain)\n",
    "        self.y_data = torch.LongTensor(ytrain)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.len = Xtest.shape[0]\n",
    "        self.X_data = torch.FloatTensor(Xtest)\n",
    "        self.y_data = torch.LongTensor(ytest)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainDataset = TrainDataset()\n",
    "TestDataset = TestDataset()\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])  # number of workers\n",
    "    print('Using {} dataloader workers every process'.format(nw))\n",
    "else:\n",
    "    nw = 0\n",
    "\n",
    "train_loader = DataLoader(TrainDataset, batch_size=batch_size, shuffle=True, num_workers=nw)\n",
    "test_loader = DataLoader(TestDataset, batch_size=batch_size, shuffle=False, num_workers=nw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型并绘制曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch[1/100] loss:1.832: 100%|██████████| 25/25 [00:27<00:00,  1.09s/it]\n",
      "train epoch[2/100] loss:2.979: 100%|██████████| 25/25 [00:27<00:00,  1.11s/it]\n",
      "train epoch[3/100] loss:1.058:  40%|████      | 10/25 [00:13<00:19,  1.32s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m outputs \u001b[38;5;241m=\u001b[39m net(inputs)\n\u001b[0;32m     36\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(outputs, labels)\n\u001b[1;32m---> 37\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# print statistics\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sendfor\\PyWORKSPACE\\stu_torch\\venv\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sendfor\\PyWORKSPACE\\stu_torch\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_name = 'HybridSN'\n",
    "net = HybridSN()\n",
    "net.to(device=device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "loss_list = []\n",
    "val_acc_list = []\n",
    "val_epoch_list = []\n",
    "\n",
    "epochs = 100\n",
    "best_acc = 0.0\n",
    "save_path = './{}Net.pth'.format(model_name)\n",
    "train_steps = len(train_loader)\n",
    "train_num = train_loader.dataset.__len__()\n",
    "val_num = test_loader.dataset.__len__()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(2,1,1)\n",
    "ax2 = fig.add_subplot(2,1,2)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # train\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    train_bar = tqdm(train_loader, file=sys.stdout)\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_bar):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        train_bar.desc = 'train epoch[{}/{}] loss:{:.3f}'.format(epoch+1, epochs, loss)\n",
    "    loss_list.append(running_loss/train_num)\n",
    "\n",
    "    # validate,after train 5 times \n",
    "    if (epoch+1)%5==0 or (epoch+1)==epochs:\n",
    "        acc = 0.0 # accumulate accurate number / epoch\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            val_bar = tqdm(test_loader, file=sys.stdout)\n",
    "\n",
    "            for batch_idx, (val_inputs, val_labels) in enumerate(val_bar):\n",
    "                val_inputs = val_inputs.to(device)\n",
    "                val_labels = val_labels.to(device)\n",
    "                outputs = net(val_inputs)\n",
    "                predict_y = torch.max(outputs, dim=1)[1] # [1]我们只需要知道最大值所在的位置在哪里，也就是索引\n",
    "                acc += torch.eq(predict_y, val_labels).sum().item()\n",
    "\n",
    "        val_accurate = acc / val_num\n",
    "        val_acc_list.append(val_accurate)\n",
    "        val_epoch_list.append(epoch)\n",
    "        print('[epoch %d] train_loss: %.3f  val_accuracy: %.3f' %\n",
    "                (epoch + 1, running_loss / train_steps, val_accurate))\n",
    "        \n",
    "        # get best model\n",
    "        if val_accurate > best_acc:\n",
    "                best_acc = val_accurate\n",
    "                torch.save(net.state_dict(), save_path)\n",
    "\n",
    "ax1.plot(np.arange(epoch+1), loss_list, label='loss')\n",
    "ax1.set_title('loss')\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(val_epoch_list, val_acc_list, label = 'val_acc', c='orange')\n",
    "ax2.set_title('acc')\n",
    "ax2.set_xlabel('epoch')\n",
    "ax2.legend()\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 载入模型验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成分类报告"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HybridSN().to(device=device)\n",
    "weights_path = save_path\n",
    "assert os.path.exists(weights_path), \"file: '{}' dose not exist.\".format(weights_path)\n",
    "model.load_state_dict(torch.load(weights_path, map_location=device))\n",
    "\n",
    "model.eval()\n",
    "y_pred_test = []\n",
    "for inputs, _ in test_loader:\n",
    "    inputs = inputs.to(device)\n",
    "    outputs = model(inputs)\n",
    "    predicted = np.argmax(outputs.detach().cpu().numpy(), axis=1)\n",
    "    y_pred_test.append(predicted)\n",
    "\n",
    "y_pred_test = np.concatenate(y_pred_test)\n",
    "\n",
    "\n",
    "classification = classification_report(ytest, y_pred_test, digits=4)\n",
    "print(classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AA_andEachClassAccuracy(confusion_matrix):\n",
    "    counter = confusion_matrix.shape[0]\n",
    "    list_diag = np.diag(confusion_matrix)\n",
    "    list_raw_sum = np.sum(confusion_matrix, axis=1)\n",
    "    each_acc = np.nan_to_num(truediv(list_diag, list_raw_sum))\n",
    "    average_acc = np.mean(each_acc)\n",
    "    return each_acc, average_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reports (test_loader,y_test,name):\n",
    "    count = 0\n",
    "    # 模型测试\n",
    "    for inputs, _ in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = net(inputs)\n",
    "        outputs = np.argmax(outputs.detach().cpu().numpy(), axis=1)\n",
    "        if count == 0:\n",
    "            y_pred = outputs\n",
    "            count = 1\n",
    "        else:\n",
    "            y_pred = np.concatenate((y_pred, outputs))\n",
    "\n",
    "    if name == 'IP':\n",
    "        target_names = ['Alfalfa', 'Corn-notill', 'Corn-mintill', 'Corn'\n",
    "                        ,'Grass-pasture', 'Grass-trees', 'Grass-pasture-mowed', \n",
    "                        'Hay-windrowed', 'Oats', 'Soybean-notill', 'Soybean-mintill',\n",
    "                        'Soybean-clean', 'Wheat', 'Woods', 'Buildings-Grass-Trees-Drives',\n",
    "                        'Stone-Steel-Towers']\n",
    "    elif name == 'SA':\n",
    "        target_names = ['Brocoli_green_weeds_1','Brocoli_green_weeds_2','Fallow','Fallow_rough_plow','Fallow_smooth',\n",
    "                        'Stubble','Celery','Grapes_untrained','Soil_vinyard_develop','Corn_senesced_green_weeds',\n",
    "                        'Lettuce_romaine_4wk','Lettuce_romaine_5wk','Lettuce_romaine_6wk','Lettuce_romaine_7wk',\n",
    "                        'Vinyard_untrained','Vinyard_vertical_trellis']\n",
    "    elif name == 'PU':\n",
    "        target_names = ['Asphalt','Meadows','Gravel','Trees', 'Painted metal sheets','Bare Soil','Bitumen',\n",
    "                        'Self-Blocking Bricks','Shadows']\n",
    "    \n",
    "    classification = classification_report(y_test, y_pred, target_names=target_names)\n",
    "    oa = accuracy_score(y_test, y_pred)\n",
    "    confusion = confusion_matrix((y_test, y_pred)\n",
    "    each_acc, aa = AA_andEachClassAccuracy(confusion)\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "    \n",
    "    return classification, confusion, oa*100, each_acc*100, aa*100, kappa*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification, confusion, oa, each_acc, aa, kappa = reports(test_loader, ytest, 'IP')\n",
    "classification = str(classification)\n",
    "confusion = str(confusion)\n",
    "file_name = \"classification_report.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_name, 'w') as x_file:\n",
    "    x_file.write('\\n')\n",
    "    x_file.write('{} Kappa accuracy (%)'.format(kappa))\n",
    "    x_file.write('\\n')\n",
    "    x_file.write('{} Overall accuracy (%)'.format(oa))\n",
    "    x_file.write('\\n')\n",
    "    x_file.write('{} Average accuracy (%)'.format(aa))\n",
    "    x_file.write('\\n')\n",
    "    x_file.write('\\n')\n",
    "    x_file.write('{}'.format(classification))\n",
    "    x_file.write('\\n')\n",
    "    x_file.write('{}'.format(confusion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Patch(data,height_index,width_index):\n",
    "    height_slice = slice(height_index, height_index+patch_size)\n",
    "    width_slice = slice(width_index, width_index+patch_size)\n",
    "    patch = data[height_slice, width_slice, :]\n",
    "    \n",
    "    return patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the original image\n",
    "X, y = loadData(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = y.shape[0]\n",
    "width = y.shape[1]\n",
    "patch_size = patch_size\n",
    "numComponents = pca_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,pca = applyPCA(X, numComponents=3)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = padWithZeros(X, patch_size//2)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#逐像素预测图像类别\n",
    "outputs = np.zeros((height,width))\n",
    "for i in range(height):\n",
    "    for j in range(width):\n",
    "        target = int(y[i,j])\n",
    "        if target == 0 :\n",
    "            continue\n",
    "        else :\n",
    "            image_patch=Patch(X,i,j)\n",
    "            X_test_image = image_patch.reshape(1,image_patch.shape[0],image_patch.shape[1], image_patch.shape[2]).astype('float32')                                   \n",
    "            prediction = (model.predict(X_test_image))\n",
    "            prediction = np.argmax(prediction, axis=1)\n",
    "            outputs[i][j] = prediction+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = spectral.imshow(classes = y,figsize =(7,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image = spectral.imshow(classes = outputs.astype(int),figsize =(7,7))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
